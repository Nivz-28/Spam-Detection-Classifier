{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Python Implementation of Naiye Bayes Algorithm"
      ],
      "metadata": {
        "id": "WK6B1bbPPvL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcq_UqNMPtZ7",
        "outputId": "7c59dff2-f9df-4949-f002-51652d5b59c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "#Import The Data\n",
        "# Replace 'your_zip_file.zip' with the name of your ZIP file\n",
        "zip_file_path = '/content/archive.zip'\n",
        "\n",
        "# Create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # You can specify the directory where you want to extract the files\n",
        "    # For example, to extract to the current working directory:\n",
        "    # zip_ref.extractall()\n",
        "\n",
        "    # Or you can specify a different directory:\n",
        "    extraction_path = '/content'\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "print(f'Files extracted to: {extraction_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBTAkG6cplcR",
        "outputId": "b924cdc2-557e-40c5-b744-fc94327eb9b2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing the data\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report as cr\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.data = None\n",
        "        self.labels = None\n",
        "        self.texts = None\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        # Stopwords and stemmer initialization\n",
        "        self.stop_words = set(stop_words)\n",
        "        self.ps = PorterStemmer()\n",
        "        self.clf = MultinomialNB()\n",
        "    def basic_tokenize(self, text):\n",
        "        return text.split()\n",
        "\n",
        "    def load_data(self):\n",
        "        # Load the CSV data\n",
        "        data = pd.read_csv(self.filepath, encoding='latin-1')\n",
        "\n",
        "        # Drop the unnecessary columns\n",
        "        data_cleaned = data.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
        "\n",
        "        # Extract labels and texts\n",
        "        self.labels = data_cleaned['v1'].values\n",
        "        self.texts = data_cleaned['v2'].values\n",
        "\n",
        "    def clean_text(self):\n",
        "        cleaned_texts = []\n",
        "        for text in self.texts:\n",
        "            # Convert to lowercase\n",
        "            text = text.lower()\n",
        "\n",
        "            # Remove punctuation and special characters\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "            # Tokenize using basic_tokenize\n",
        "            tokens = self.basic_tokenize(text)\n",
        "\n",
        "            # Remove stopwords and perform stemming\n",
        "            tokens = [self.ps.stem(word) for word in tokens if word not in self.stop_words]\n",
        "\n",
        "            # Join tokens back to a single string\n",
        "            cleaned_texts.append(' '.join(tokens))\n",
        "\n",
        "        self.texts = cleaned_texts\n",
        "\n",
        "    def encode_labels(self):\n",
        "        # Using a dictionary to map labels to their encoded values\n",
        "        label_mapping = {'ham': 0, 'spam': 1}\n",
        "        self.labels = [label_mapping[label] for label in self.labels]\n",
        "\n",
        "    def tokenize_bow(self):\n",
        "        vectorizer_bow = CountVectorizer()\n",
        "        bow_representation = vectorizer_bow.fit_transform(self.texts)\n",
        "        return bow_representation.toarray()\n",
        "\n",
        "\n",
        "    def split_data(self, test_size=0.2):\n",
        "        \"\"\"Split the data into training and test sets.\"\"\"\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.texts,\n",
        "            self.labels,\n",
        "            test_size=test_size,\n",
        "            stratify=self.labels,\n",
        "            random_state=42  # Setting a random state for reproducibility\n",
        "        )\n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
        "\n",
        "\n",
        "class TextClassifier(TextPreprocessor):\n",
        "    def __init__(self, filepath):\n",
        "        super().__init__(filepath)\n",
        "        self.clf = MultinomialNB()\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "    def train_classifier(self):\n",
        "        \"\"\"Train the Multinomial Naive Bayes classifier.\"\"\"\n",
        "        # Convert texts to BoW representation for training\n",
        "        X_train_bow = self.vectorizer.fit_transform(self.X_train)\n",
        "        self.clf.fit(X_train_bow, self.y_train)\n",
        "\n",
        "    def predict(self, texts=None):\n",
        "        \"\"\"Predict the labels for the provided texts.\"\"\"\n",
        "        if texts is None:\n",
        "            texts = self.X_test\n",
        "\n",
        "        texts_bow = self.vectorizer.transform(texts)\n",
        "        return self.clf.predict(texts_bow)\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate the classifier's performance.\"\"\"\n",
        "        y_pred = self.predict()\n",
        "\n",
        "        # Performance metrics\n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        classification_rep = cr(self.y_test, y_pred)  # Using the explicitly imported function\n",
        "        conf_matrix = confusion_matrix(self.y_test, y_pred)\n",
        "\n",
        "        return accuracy, classification_rep, conf_matrix\n",
        "\n",
        "    def decode_labels(self, encoded_labels):\n",
        "        # Reverse the original mapping\n",
        "        reverse_mapping = {0: 'ham', 1: 'spam'}\n",
        "\n",
        "        # Convert each encoded label to its original label\n",
        "        decoded_labels = [reverse_mapping[label] for label in encoded_labels]\n",
        "\n",
        "        return decoded_labels\n",
        ""
      ],
      "metadata": {
        "id": "HSnfH41OjrFD"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the text classifier\n",
        "classifier = TextClassifier('/content/spam.csv')\n",
        "classifier.load_data()\n",
        "\n",
        "# Preprocess the texts and encode labels\n",
        "classifier.clean_text()\n",
        "classifier.encode_labels()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "classifier.split_data()\n",
        "classifier.train_classifier()\n",
        "accuracy, classification_rep, conf_matrix = classifier.evaluate()\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykDi7LGg0ftv",
        "outputId": "c706e671-445b-47f8-c569-3817a449a4ea"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.03%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       966\n",
            "           1       0.97      0.88      0.92       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.98      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[962   4]\n",
            " [ 18 131]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels for the first 20 test messages\n",
        "#sample_texts_1 = classifier.X_test[:10]  [uncomment this if you want predictions on x_test]\n",
        "\n",
        "#comment random_test_sentences and sample_texts if you uncommenting the above one\n",
        "random_test_sentences = [\n",
        "    \"You've won a million dollars! Click the link to claim your prize now!\",\n",
        "    \"Hi there, this is a friendly reminder about our meeting tomorrow at 3 PM.\",\n",
        "    \"Your Amazon order #123456 is on its way. Track your shipment here: amazon.com/track\",\n",
        "    \"URGENT: Your bank account has been locked. Please call our customer service to resolve this issue.\",\n",
        "    \"Congratulations! You are the 100,000th visitor to our website. Claim your free gift!\",\n",
        "    \"Your Apple ID has been compromised. Click this link to verify your account.\",\n",
        "    \"Dear customer, your subscription will expire in 3 days. Renew now to continue using our service.\",\n",
        "    \"You've been selected for a job interview. Please confirm your availability.\",\n",
        "    \"Get a free iPhone by participating in our survey!\"\n",
        "]\n",
        "sample_texts = random_test_sentences\n",
        "predicted_labels = classifier.predict(sample_texts)\n",
        "\n",
        "# Decode the predicted labels (using the decode_labels function from your previous code)\n",
        "decoded_sample = classifier.decode_labels(predicted_labels)\n",
        "\n",
        "# Print the samples, predicted labels, and decoded samples together\n",
        "for i in range(len(sample_texts)):\n",
        "    print(\"\\nSample Text:\", sample_texts[i])\n",
        "    print(\"Predicted Label (Decoded):\", decoded_sample[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKqPU7Yz4yQi",
        "outputId": "b899e1d5-1db9-4bcf-93fe-b44bc8074101"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Text: You've won a million dollars! Click the link to claim your prize now!\n",
            "Predicted Label (Decoded): spam\n",
            "\n",
            "Sample Text: Hi there, this is a friendly reminder about our meeting tomorrow at 3 PM.\n",
            "Predicted Label (Decoded): ham\n",
            "\n",
            "Sample Text: Your Amazon order #123456 is on its way. Track your shipment here: amazon.com/track\n",
            "Predicted Label (Decoded): ham\n",
            "\n",
            "Sample Text: URGENT: Your bank account has been locked. Please call our customer service to resolve this issue.\n",
            "Predicted Label (Decoded): spam\n",
            "\n",
            "Sample Text: Congratulations! You are the 100,000th visitor to our website. Claim your free gift!\n",
            "Predicted Label (Decoded): spam\n",
            "\n",
            "Sample Text: Your Apple ID has been compromised. Click this link to verify your account.\n",
            "Predicted Label (Decoded): ham\n",
            "\n",
            "Sample Text: Dear customer, your subscription will expire in 3 days. Renew now to continue using our service.\n",
            "Predicted Label (Decoded): ham\n",
            "\n",
            "Sample Text: You've been selected for a job interview. Please confirm your availability.\n",
            "Predicted Label (Decoded): ham\n",
            "\n",
            "Sample Text: Get a free iPhone by participating in our survey!\n",
            "Predicted Label (Decoded): spam\n"
          ]
        }
      ]
    }
  ]
}